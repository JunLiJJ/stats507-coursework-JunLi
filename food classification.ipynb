{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train size: 75750\n",
      "Original validation size: 25250\n",
      "Reduced train size: 7575\n",
      "Reduced validation size: 2525\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# 加载完整数据集\n",
    "ds = load_dataset(\"ethz/food101\")\n",
    "train_num = ds[\"train\"].num_rows\n",
    "valid_num = ds[\"validation\"].num_rows\n",
    "\n",
    "# 假设想要取 1/20 的数据\n",
    "fraction = 1/10\n",
    "\n",
    "# 计算要选取的样本数\n",
    "train_sample_size = int(train_num * fraction)\n",
    "valid_sample_size = int(valid_num * fraction)\n",
    "\n",
    "# 对 train 和 validation 数据集进行随机打乱并取子集\n",
    "train_subset = ds[\"train\"].shuffle(seed=42).select(range(train_sample_size))\n",
    "valid_subset = ds[\"validation\"].shuffle(seed=42).select(range(valid_sample_size))\n",
    "\n",
    "# 将抽取后的子集组成新的 DatasetDict\n",
    "ds = DatasetDict({\n",
    "    \"train\": train_subset,\n",
    "    \"validation\": valid_subset\n",
    "})\n",
    "\n",
    "print(\"Original train size:\", train_num)\n",
    "print(\"Original validation size:\", valid_num)\n",
    "print(\"Reduced train size:\", ds[\"train\"].num_rows)\n",
    "print(\"Reduced validation size:\", ds[\"validation\"].num_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 7575\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 2525\n",
      "    })\n",
      "})\n",
      "{'image': <PIL.Image.Image image mode=RGB size=512x512 at 0x3FBA0BE00>, 'label': 71}\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x504 at 0x3FB95F920>, 'label': 28}\n"
     ]
    }
   ],
   "source": [
    "# 查看数据集的结构\n",
    "print(ds)\n",
    "\n",
    "# 查看训练集的前几条数据\n",
    "print(ds['train'][1])\n",
    "\n",
    "# 查看测试集的前几条数据\n",
    "print(ds['validation'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, RandomResizedCrop, RandomHorizontalFlip, ColorJitter, ToTensor, Normalize\n",
    "\n",
    "# 数据增强\n",
    "transform = Compose([\n",
    "    RandomResizedCrop(224),\n",
    "    RandomHorizontalFlip(),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    # Check and handle unexpected data types\n",
    "    if isinstance(example['image'], list):\n",
    "        example['pixel_values'] = [transform(img) for img in example['image']]\n",
    "    else:\n",
    "        example['pixel_values'] = transform(example['image'])\n",
    "    \n",
    "    return example\n",
    "\n",
    "ds.reset_format()  # Ensure dataset is in the original format\n",
    "ds = ds.with_transform(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 访问单个样本\n",
    "sample = ds['train'][0]\n",
    "# 此时才会调用 preprocess 函数并打印出 Inside preprocess 信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example['pixel_values'] for example in examples])\n",
    "    labels = torch.tensor([example['label'] for example in examples])\n",
    "    return {'pixel_values': pixel_values, 'label': labels}\n",
    "\n",
    "train_loader = DataLoader(ds['train'], batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(ds['validation'], batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "\n",
    "# 下载预训练的ViT模型（ImageNet上预训练）\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=101,  # Food101有101个类别\n",
    ")\n",
    "\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器和调度器\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=1e-8)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/xjzf7_dx3qs2n0t440x90twh0000gn/T/ipykernel_94095/3946530179.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/xjzf7_dx3qs2n0t440x90twh0000gn/T/ipykernel_94095/3946530179.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/237, Loss: 2.3157\n",
      "Batch 10/237, Loss: 1.8177\n",
      "Batch 20/237, Loss: 1.7379\n",
      "Batch 30/237, Loss: 1.8367\n",
      "Batch 40/237, Loss: 1.4568\n",
      "Batch 50/237, Loss: 1.2942\n",
      "Batch 60/237, Loss: 1.0081\n",
      "Batch 70/237, Loss: 1.0331\n",
      "Batch 80/237, Loss: 1.1565\n",
      "Batch 90/237, Loss: 0.9708\n",
      "Batch 100/237, Loss: 1.3464\n",
      "Batch 110/237, Loss: 1.7221\n",
      "Batch 120/237, Loss: 1.1477\n",
      "Batch 130/237, Loss: 1.3128\n",
      "Batch 140/237, Loss: 1.1915\n",
      "Batch 150/237, Loss: 1.3935\n",
      "Batch 160/237, Loss: 1.0785\n",
      "Batch 170/237, Loss: 0.8996\n",
      "Batch 180/237, Loss: 1.1412\n",
      "Batch 190/237, Loss: 0.9822\n",
      "Batch 200/237, Loss: 0.7510\n",
      "Batch 210/237, Loss: 1.1526\n",
      "Batch 220/237, Loss: 0.9490\n",
      "Batch 230/237, Loss: 1.3073\n",
      "Epoch 1 Completed. Average Training Loss: 1.3129\n",
      "Validation Loss: 1.1005, Accuracy: 0.7303\n",
      "Epoch 2/3\n",
      "Batch 0/237, Loss: 1.1009\n",
      "Batch 10/237, Loss: 1.6191\n",
      "Batch 20/237, Loss: 0.8740\n",
      "Batch 30/237, Loss: 1.2160\n",
      "Batch 40/237, Loss: 1.4940\n",
      "Batch 50/237, Loss: 1.0368\n",
      "Batch 60/237, Loss: 0.7844\n",
      "Batch 70/237, Loss: 0.8926\n",
      "Batch 80/237, Loss: 0.8064\n",
      "Batch 90/237, Loss: 0.8946\n",
      "Batch 100/237, Loss: 0.9683\n",
      "Batch 110/237, Loss: 1.4079\n",
      "Batch 120/237, Loss: 1.1318\n",
      "Batch 130/237, Loss: 1.0134\n",
      "Batch 140/237, Loss: 1.1335\n",
      "Batch 150/237, Loss: 1.3214\n",
      "Batch 160/237, Loss: 1.3739\n",
      "Batch 170/237, Loss: 1.2701\n",
      "Batch 180/237, Loss: 0.7000\n",
      "Batch 190/237, Loss: 0.8276\n",
      "Batch 200/237, Loss: 0.6166\n",
      "Batch 210/237, Loss: 0.9075\n",
      "Batch 220/237, Loss: 0.9631\n",
      "Batch 230/237, Loss: 0.9336\n",
      "Epoch 2 Completed. Average Training Loss: 1.0193\n",
      "Validation Loss: 1.2176, Accuracy: 0.7069\n",
      "Epoch 3/3\n",
      "Batch 0/237, Loss: 1.1551\n",
      "Batch 10/237, Loss: 1.2746\n",
      "Batch 20/237, Loss: 1.1491\n",
      "Batch 30/237, Loss: 1.4494\n",
      "Batch 40/237, Loss: 0.5751\n",
      "Batch 50/237, Loss: 0.4425\n",
      "Batch 60/237, Loss: 1.0450\n",
      "Batch 70/237, Loss: 0.9509\n",
      "Batch 80/237, Loss: 0.6438\n",
      "Batch 90/237, Loss: 0.7194\n",
      "Batch 100/237, Loss: 0.5711\n",
      "Batch 110/237, Loss: 1.1552\n",
      "Batch 120/237, Loss: 0.9608\n",
      "Batch 130/237, Loss: 0.7284\n",
      "Batch 140/237, Loss: 0.7189\n",
      "Batch 150/237, Loss: 0.8907\n",
      "Batch 160/237, Loss: 1.0051\n",
      "Batch 170/237, Loss: 1.0837\n",
      "Batch 180/237, Loss: 0.8485\n",
      "Batch 190/237, Loss: 1.0338\n",
      "Batch 200/237, Loss: 1.0483\n",
      "Batch 210/237, Loss: 0.9788\n",
      "Batch 220/237, Loss: 0.6741\n",
      "Batch 230/237, Loss: 0.5149\n",
      "Epoch 3 Completed. Average Training Loss: 0.8460\n",
      "Validation Loss: 1.2750, Accuracy: 0.6923\n",
      "\n",
      "Training Summary:\n",
      "Epoch     Training Loss  Validation Loss     Accuracy  \n",
      "1         1.3129         1.1005              0.7303    \n",
      "2         1.0193         1.2176              0.7069    \n",
      "3         0.8460         1.2750              0.6923    \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 假设 train_loader 和 val_loader 已经初始化\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 定义模型、优化器、损失函数\n",
    "model = AutoModelForImageClassification.from_pretrained(\"nateraw/food\").to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=0.0002)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 超参数\n",
    "epochs = 3\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "log_every = 10  # 每多少个 batch 打印一次日志\n",
    "\n",
    "# 存储结果\n",
    "results = {\"epoch\": [], \"step\": [], \"train_loss\": [], \"val_loss\": [], \"accuracy\": []}\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        total_steps += 1\n",
    "\n",
    "        # 打印训练进度\n",
    "        if batch_idx % log_every == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / total_steps\n",
    "    print(f\"Epoch {epoch} Completed. Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # 保存结果\n",
    "    results[\"epoch\"].append(epoch)\n",
    "    results[\"step\"].append(total_steps)\n",
    "    results[\"train_loss\"].append(avg_train_loss)\n",
    "    results[\"val_loss\"].append(avg_val_loss)\n",
    "    results[\"accuracy\"].append(accuracy)\n",
    "\n",
    "# 打印总结\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"{'Epoch':<10}{'Training Loss':<15}{'Validation Loss':<20}{'Accuracy':<10}\")\n",
    "for i in range(epochs):\n",
    "    print(f\"{results['epoch'][i]:<10}{results['train_loss'][i]:<15.4f}{results['val_loss'][i]:<20.4f}{results['accuracy'][i]:<10.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pixel_values', 'label'])\n"
     ]
    }
   ],
   "source": [
    "print(batch.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 69.35%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        # 获取输入和标签\n",
    "        images = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        # 获取模型输出并提取 logits\n",
    "        outputs = model(images)\n",
    "        logits = outputs.logits  # 提取 logits\n",
    "        \n",
    "        # 使用 logits 进行预测\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        # 计算总数和正确预测数\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# 打印验证集的准确率\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 15\u001b[0m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForImageClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m     12\u001b[0m processor \u001b[38;5;241m=\u001b[39m AutoImageProcessor\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_directory\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.jpeg\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     17\u001b[0m         image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_directory, filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Images'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "image_directory = \"Images\"\n",
    "\n",
    "\n",
    "model_name = \"nateraw/food\"\n",
    "model = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "for filename in os.listdir(image_directory):\n",
    "    if filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "        image_path = os.path.join(image_directory, filename)\n",
    "\n",
    "\n",
    "        image = Image.open(image_path)\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "\n",
    "        predicted_class_idx = outputs.logits.argmax(-1).item()\n",
    "        predicted_label = model.config.id2label[predicted_class_idx]\n",
    "\n",
    "        print(predicted_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
