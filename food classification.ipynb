{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train size: 75750\n",
      "Original validation size: 25250\n",
      "Reduced train size: 7575\n",
      "Reduced validation size: 2525\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# 加载完整数据集\n",
    "ds = load_dataset(\"ethz/food101\")\n",
    "train_num = ds[\"train\"].num_rows\n",
    "valid_num = ds[\"validation\"].num_rows\n",
    "\n",
    "# 假设想要取 1/20 的数据\n",
    "fraction = 1/10\n",
    "\n",
    "# 计算要选取的样本数\n",
    "train_sample_size = int(train_num * fraction)\n",
    "valid_sample_size = int(valid_num * fraction)\n",
    "\n",
    "# 对 train 和 validation 数据集进行随机打乱并取子集\n",
    "train_subset = ds[\"train\"].shuffle(seed=42).select(range(train_sample_size))\n",
    "valid_subset = ds[\"validation\"].shuffle(seed=42).select(range(valid_sample_size))\n",
    "\n",
    "# 将抽取后的子集组成新的 DatasetDict\n",
    "ds = DatasetDict({\n",
    "    \"train\": train_subset,\n",
    "    \"validation\": valid_subset\n",
    "})\n",
    "\n",
    "print(\"Original train size:\", train_num)\n",
    "print(\"Original validation size:\", valid_num)\n",
    "print(\"Reduced train size:\", ds[\"train\"].num_rows)\n",
    "print(\"Reduced validation size:\", ds[\"validation\"].num_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 7575\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['image', 'label'],\n",
      "        num_rows: 2525\n",
      "    })\n",
      "})\n",
      "{'image': <PIL.Image.Image image mode=RGB size=512x512 at 0x3FF4C4230>, 'label': 71}\n",
      "{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x504 at 0x3FE2C4C50>, 'label': 28}\n"
     ]
    }
   ],
   "source": [
    "# 查看数据集的结构\n",
    "print(ds)\n",
    "\n",
    "# 查看训练集的前几条数据\n",
    "print(ds['train'][1])\n",
    "\n",
    "# 查看测试集的前几条数据\n",
    "print(ds['validation'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose, RandomResizedCrop, RandomHorizontalFlip, ColorJitter, ToTensor, Normalize\n",
    "\n",
    "# 数据增强\n",
    "transform = Compose([\n",
    "    RandomResizedCrop(224),\n",
    "    RandomHorizontalFlip(),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "def preprocess(example):\n",
    "    # Check and handle unexpected data types\n",
    "    if isinstance(example['image'], list):\n",
    "        example['pixel_values'] = [transform(img) for img in example['image']]\n",
    "    else:\n",
    "        example['pixel_values'] = transform(example['image'])\n",
    "    \n",
    "    return example\n",
    "\n",
    "ds.reset_format()  # Ensure dataset is in the original format\n",
    "ds = ds.with_transform(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 访问单个样本\n",
    "sample = ds['train'][0]\n",
    "# 此时才会调用 preprocess 函数并打印出 Inside preprocess 信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(examples):\n",
    "    pixel_values = torch.stack([example['pixel_values'] for example in examples])\n",
    "    labels = torch.tensor([example['label'] for example in examples])\n",
    "    return {'pixel_values': pixel_values, 'label': labels}\n",
    "\n",
    "train_loader = DataLoader(ds['train'], batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(ds['validation'], batch_size=32, shuffle=False, collate_fn=collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, ViTForImageClassification\n",
    "\n",
    "# 下载预训练的ViT模型（ImageNet上预训练）\n",
    "model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=101,  # Food101有101个类别\n",
    ")\n",
    "\n",
    "model = model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器和调度器\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = CosineAnnealingLR(optimizer, T_max=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/xjzf7_dx3qs2n0t440x90twh0000gn/T/ipykernel_94095/2937678325.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/xjzf7_dx3qs2n0t440x90twh0000gn/T/ipykernel_94095/2937678325.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/237, Loss: 1.4607\n",
      "Batch 10/237, Loss: 1.8996\n",
      "Batch 20/237, Loss: 2.0092\n",
      "Batch 30/237, Loss: 1.4960\n",
      "Batch 40/237, Loss: 1.6364\n",
      "Batch 50/237, Loss: 2.1042\n",
      "Batch 60/237, Loss: 0.8125\n",
      "Batch 70/237, Loss: 0.8102\n",
      "Batch 80/237, Loss: 1.1118\n",
      "Batch 90/237, Loss: 1.2422\n",
      "Batch 100/237, Loss: 1.2459\n",
      "Batch 110/237, Loss: 1.0719\n",
      "Batch 120/237, Loss: 1.0350\n",
      "Batch 130/237, Loss: 0.8642\n",
      "Batch 140/237, Loss: 1.0745\n",
      "Batch 150/237, Loss: 1.8133\n",
      "Batch 160/237, Loss: 1.0905\n",
      "Batch 170/237, Loss: 1.1453\n",
      "Batch 180/237, Loss: 0.9390\n",
      "Batch 190/237, Loss: 1.9917\n",
      "Batch 200/237, Loss: 1.3051\n",
      "Batch 210/237, Loss: 1.2332\n",
      "Batch 220/237, Loss: 1.3495\n",
      "Batch 230/237, Loss: 1.3919\n",
      "Epoch 1 Completed. Average Training Loss: 1.3253\n",
      "Validation Loss: 1.1283, Accuracy: 0.7172\n",
      "Epoch 2/5\n",
      "Batch 0/237, Loss: 0.9801\n",
      "Batch 10/237, Loss: 0.6122\n",
      "Batch 20/237, Loss: 1.0358\n",
      "Batch 30/237, Loss: 0.8174\n",
      "Batch 40/237, Loss: 1.2123\n",
      "Batch 50/237, Loss: 0.9493\n",
      "Batch 60/237, Loss: 1.2787\n",
      "Batch 70/237, Loss: 0.8445\n",
      "Batch 80/237, Loss: 1.3774\n",
      "Batch 90/237, Loss: 0.8432\n",
      "Batch 100/237, Loss: 1.2045\n",
      "Batch 110/237, Loss: 0.6983\n",
      "Batch 120/237, Loss: 1.0299\n",
      "Batch 130/237, Loss: 0.5701\n",
      "Batch 140/237, Loss: 0.8173\n",
      "Batch 150/237, Loss: 1.3642\n",
      "Batch 160/237, Loss: 1.3069\n",
      "Batch 170/237, Loss: 1.1739\n",
      "Batch 180/237, Loss: 1.2958\n",
      "Batch 190/237, Loss: 1.2044\n",
      "Batch 200/237, Loss: 0.8174\n",
      "Batch 210/237, Loss: 0.9922\n",
      "Batch 220/237, Loss: 0.8697\n",
      "Batch 230/237, Loss: 1.1608\n",
      "Epoch 2 Completed. Average Training Loss: 1.0036\n",
      "Validation Loss: 1.1552, Accuracy: 0.7073\n",
      "Epoch 3/5\n",
      "Batch 0/237, Loss: 0.6320\n",
      "Batch 10/237, Loss: 1.0847\n",
      "Batch 20/237, Loss: 0.2858\n",
      "Batch 30/237, Loss: 1.2470\n",
      "Batch 40/237, Loss: 1.0051\n",
      "Batch 50/237, Loss: 0.5884\n",
      "Batch 60/237, Loss: 0.6794\n",
      "Batch 70/237, Loss: 0.6053\n",
      "Batch 80/237, Loss: 0.7281\n",
      "Batch 90/237, Loss: 0.9808\n",
      "Batch 100/237, Loss: 0.6906\n",
      "Batch 110/237, Loss: 1.0606\n",
      "Batch 120/237, Loss: 0.9281\n",
      "Batch 130/237, Loss: 0.9264\n",
      "Batch 140/237, Loss: 0.4305\n",
      "Batch 150/237, Loss: 0.8948\n",
      "Batch 160/237, Loss: 0.5929\n",
      "Batch 170/237, Loss: 1.3532\n",
      "Batch 180/237, Loss: 1.1202\n",
      "Batch 190/237, Loss: 1.1122\n",
      "Batch 200/237, Loss: 1.0970\n",
      "Batch 210/237, Loss: 0.7088\n",
      "Batch 220/237, Loss: 0.7586\n",
      "Batch 230/237, Loss: 0.9988\n",
      "Epoch 3 Completed. Average Training Loss: 0.8771\n",
      "Validation Loss: 1.1416, Accuracy: 0.7180\n",
      "Epoch 4/5\n",
      "Batch 0/237, Loss: 0.8182\n",
      "Batch 10/237, Loss: 1.1986\n",
      "Batch 20/237, Loss: 0.5946\n",
      "Batch 30/237, Loss: 0.2924\n",
      "Batch 40/237, Loss: 0.6673\n",
      "Batch 50/237, Loss: 0.8000\n",
      "Batch 60/237, Loss: 0.7666\n",
      "Batch 70/237, Loss: 0.6955\n",
      "Batch 80/237, Loss: 0.7536\n",
      "Batch 90/237, Loss: 0.7776\n",
      "Batch 100/237, Loss: 0.4765\n",
      "Batch 110/237, Loss: 0.6605\n",
      "Batch 120/237, Loss: 0.8017\n",
      "Batch 130/237, Loss: 0.7829\n",
      "Batch 140/237, Loss: 1.1472\n",
      "Batch 150/237, Loss: 1.2580\n",
      "Batch 160/237, Loss: 0.6655\n",
      "Batch 170/237, Loss: 1.1109\n",
      "Batch 180/237, Loss: 1.0608\n",
      "Batch 190/237, Loss: 0.3668\n",
      "Batch 200/237, Loss: 0.9400\n",
      "Batch 210/237, Loss: 0.9079\n",
      "Batch 220/237, Loss: 1.3690\n",
      "Batch 230/237, Loss: 1.0674\n",
      "Epoch 4 Completed. Average Training Loss: 0.7424\n",
      "Validation Loss: 1.0653, Accuracy: 0.7287\n",
      "Epoch 5/5\n",
      "Batch 0/237, Loss: 0.5835\n",
      "Batch 10/237, Loss: 1.1184\n",
      "Batch 20/237, Loss: 0.5757\n",
      "Batch 30/237, Loss: 0.8029\n",
      "Batch 40/237, Loss: 0.3505\n",
      "Batch 50/237, Loss: 0.7805\n",
      "Batch 60/237, Loss: 0.2802\n",
      "Batch 70/237, Loss: 0.9360\n",
      "Batch 80/237, Loss: 0.4417\n",
      "Batch 90/237, Loss: 0.4447\n",
      "Batch 100/237, Loss: 0.6530\n",
      "Batch 110/237, Loss: 0.7999\n",
      "Batch 120/237, Loss: 0.7410\n",
      "Batch 130/237, Loss: 0.9274\n",
      "Batch 140/237, Loss: 0.6828\n",
      "Batch 150/237, Loss: 1.0113\n",
      "Batch 160/237, Loss: 0.8358\n",
      "Batch 170/237, Loss: 0.7240\n",
      "Batch 180/237, Loss: 0.9822\n",
      "Batch 190/237, Loss: 0.6454\n",
      "Batch 200/237, Loss: 0.6374\n",
      "Batch 210/237, Loss: 0.4756\n",
      "Batch 220/237, Loss: 0.5650\n",
      "Batch 230/237, Loss: 0.5311\n",
      "Epoch 5 Completed. Average Training Loss: 0.6745\n",
      "Validation Loss: 1.1179, Accuracy: 0.7244\n",
      "\n",
      "Training Summary:\n",
      "Epoch     Training Loss  Validation Loss     Accuracy  \n",
      "1         1.3253         1.1283              0.7172    \n",
      "2         1.0036         1.1552              0.7073    \n",
      "3         0.8771         1.1416              0.7180    \n",
      "4         0.7424         1.0653              0.7287    \n",
      "5         0.6745         1.1179              0.7244    \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 假设 train_loader 和 val_loader 已经初始化\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "scaler = GradScaler()\n",
    "\n",
    "# 定义模型、优化器、损失函数\n",
    "model = AutoModelForImageClassification.from_pretrained(\"nateraw/food\").to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=0.0002)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# 超参数\n",
    "epochs = 5\n",
    "train_batch_size = 128\n",
    "eval_batch_size = 128\n",
    "log_every = 10  # 每多少个 batch 打印一次日志\n",
    "\n",
    "# 存储结果\n",
    "results = {\"epoch\": [], \"step\": [], \"train_loss\": [], \"val_loss\": [], \"accuracy\": []}\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(f\"Epoch {epoch}/{epochs}\")\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    total_steps = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        inputs = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        total_steps += 1\n",
    "\n",
    "        # 打印训练进度\n",
    "        if batch_idx % log_every == 0:\n",
    "            print(f\"Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_train_loss = total_train_loss / total_steps\n",
    "    print(f\"Epoch {epoch} Completed. Average Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # 验证阶段\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            inputs = batch[\"pixel_values\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs.logits, labels)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs.logits, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    accuracy = correct / total\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    # 保存结果\n",
    "    results[\"epoch\"].append(epoch)\n",
    "    results[\"step\"].append(total_steps)\n",
    "    results[\"train_loss\"].append(avg_train_loss)\n",
    "    results[\"val_loss\"].append(avg_val_loss)\n",
    "    results[\"accuracy\"].append(accuracy)\n",
    "\n",
    "# 打印总结\n",
    "print(\"\\nTraining Summary:\")\n",
    "print(f\"{'Epoch':<10}{'Training Loss':<15}{'Validation Loss':<20}{'Accuracy':<10}\")\n",
    "for i in range(epochs):\n",
    "    print(f\"{results['epoch'][i]:<10}{results['train_loss'][i]:<15.4f}{results['val_loss'][i]:<20.4f}{results['accuracy'][i]:<10.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['pixel_values', 'label'])\n"
     ]
    }
   ],
   "source": [
    "print(batch.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 72.87%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        # 获取输入和标签\n",
    "        images = batch[\"pixel_values\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        # 获取模型输出并提取 logits\n",
    "        outputs = model(images)\n",
    "        logits = outputs.logits  # 提取 logits\n",
    "        \n",
    "        # 使用 logits 进行预测\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        \n",
    "        # 计算总数和正确预测数\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# 打印验证集的准确率\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from transformers import AutoModelForImageClassification, AutoImageProcessor\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "\n",
    "\n",
    "# image_directory = \"Images\"\n",
    "\n",
    "\n",
    "# model_name = \"nateraw/food\"\n",
    "# model = AutoModelForImageClassification.from_pretrained(model_name)\n",
    "# processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# for filename in os.listdir(image_directory):\n",
    "#     if filename.endswith(\".jpeg\") or filename.endswith(\".png\"):\n",
    "#         image_path = os.path.join(image_directory, filename)\n",
    "\n",
    "\n",
    "#         image = Image.open(image_path)\n",
    "#         inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "\n",
    "\n",
    "#         predicted_class_idx = outputs.logits.argmax(-1).item()\n",
    "#         predicted_label = model.config.id2label[predicted_class_idx]\n",
    "\n",
    "#         print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
